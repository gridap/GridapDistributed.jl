var documenterSearchIndex = {"docs":
[{"location":"Geometry/#Geometry","page":"Geometry","title":"Geometry","text":"","category":"section"},{"location":"Geometry/","page":"Geometry","title":"Geometry","text":"Modules = [GridapDistributed]\nPages   = [\"Geometry.jl\"]","category":"page"},{"location":"Geometry/#GridapDistributed.DistributedDiscreteModel","page":"Geometry","title":"GridapDistributed.DistributedDiscreteModel","text":"\n\n\n\n","category":"type"},{"location":"Geometry/#GridapDistributed.DistributedFaceLabeling","page":"Geometry","title":"GridapDistributed.DistributedFaceLabeling","text":"\n\n\n\n","category":"type"},{"location":"Geometry/#GridapDistributed.DistributedGrid","page":"Geometry","title":"GridapDistributed.DistributedGrid","text":"\n\n\n\n","category":"type"},{"location":"Geometry/#GridapDistributed.DistributedGridTopology","page":"Geometry","title":"GridapDistributed.DistributedGridTopology","text":"\n\n\n\n","category":"type"},{"location":"Geometry/#GridapDistributed.DistributedTriangulation","page":"Geometry","title":"GridapDistributed.DistributedTriangulation","text":"\n\n\n\n","category":"type"},{"location":"Geometry/#GridapDistributed.GenericDistributedDiscreteModel","page":"Geometry","title":"GridapDistributed.GenericDistributedDiscreteModel","text":"\n\n\n\n","category":"type"},{"location":"getting-started/#Getting-Started","page":"Getting Started","title":"Getting Started","text":"","category":"section"},{"location":"getting-started/#Installation-requirements","page":"Getting Started","title":"Installation requirements","text":"","category":"section"},{"location":"getting-started/","page":"Getting Started","title":"Getting Started","text":"GridapDistributed is tested on Linux, but it should be also possible to use it on Mac OS and Windows since it is written exclusively in Julia and it only depends on registered Julia packages.","category":"page"},{"location":"getting-started/#Installation","page":"Getting Started","title":"Installation","text":"","category":"section"},{"location":"getting-started/","page":"Getting Started","title":"Getting Started","text":"GridapDistributed is a registered package. Thus, the installation should be straight forward using the Julia's package manager Pkg. To this end, open the Julia REPL (i.e., execute the julia binary), type ] to enter package mode, and install GridapDistributed as follows","category":"page"},{"location":"getting-started/","page":"Getting Started","title":"Getting Started","text":"pkg> add GridapDistributed","category":"page"},{"location":"getting-started/","page":"Getting Started","title":"Getting Started","text":"You will also need the PartitionedArrays package. ","category":"page"},{"location":"getting-started/","page":"Getting Started","title":"Getting Started","text":"pkg> add PartitionedArrays","category":"page"},{"location":"getting-started/","page":"Getting Started","title":"Getting Started","text":"If you want to leverage the satellite packages of GridapDistributed.jl, i.e., GridapPETSc.jl, GridapP4est.jl, and/or GridapGmsh.jl, you have to install them separately as ","category":"page"},{"location":"getting-started/","page":"Getting Started","title":"Getting Started","text":"pkg> add GridapPETSc","category":"page"},{"location":"getting-started/","page":"Getting Started","title":"Getting Started","text":"pkg> add GridapGmsh","category":"page"},{"location":"getting-started/","page":"Getting Started","title":"Getting Started","text":"pkg> add GridapP4est","category":"page"},{"location":"getting-started/","page":"Getting Started","title":"Getting Started","text":"Please note that these three packages depend on binary builds of the corresponding libraries they wrap (i.e., PETSc, Gmsh, and P4est). By default, they will leverage binary builds available at the Julia package registry. However, one may also use custom installations of these libraries. See the documentation of the corresponding package for more details","category":"page"},{"location":"getting-started/","page":"Getting Started","title":"Getting Started","text":"For further information about how to install and manage Julia packages, see the Pkg documentation.","category":"page"},{"location":"getting-started/#Further-steps","page":"Getting Started","title":"Further steps","text":"","category":"section"},{"location":"getting-started/","page":"Getting Started","title":"Getting Started","text":"If you are new to the Gridap ecosystem of packages, we recommend that you first follow the Gridap Tutorials step by step in order to get familiar with the Gridap.jl library. GridapDistributed.jl and Gridap.jl share almost the same high-level API. Therefore, some familiarity with Gridap.jl is highly recommended (if not essential) before starting with GridapDistributed.jl. ","category":"page"},{"location":"getting-started/","page":"Getting Started","title":"Getting Started","text":"If you are already familiarized with Gridap.jl, we recommend you to start straight away with the following tutorial.","category":"page"},{"location":"MultiField/#MultiField","page":"MultiField","title":"MultiField","text":"","category":"section"},{"location":"MultiField/","page":"MultiField","title":"MultiField","text":"Modules = [GridapDistributed]\nPages   = [\"MultiField.jl\",\"BlockPartitionedArrays.jl\"]","category":"page"},{"location":"MultiField/#GridapDistributed.DistributedMultiFieldFESpace","page":"MultiField","title":"GridapDistributed.DistributedMultiFieldFESpace","text":"\n\n\n\n","category":"type"},{"location":"MultiField/#GridapDistributed.BlockPArray","page":"MultiField","title":"GridapDistributed.BlockPArray","text":"struct BlockPArray{V,T,N,A,B} <: BlockArrays.AbstractBlockArray{T,N}\n\n\n\n\n\n","category":"type"},{"location":"MultiField/#GridapDistributed.BlockPRange","page":"MultiField","title":"GridapDistributed.BlockPRange","text":"struct BlockPRange{A} <: AbstractUnitRange{Int}\n\n\n\n\n\n","category":"type"},{"location":"CellData/#CellData","page":"CellData","title":"CellData","text":"","category":"section"},{"location":"CellData/","page":"CellData","title":"CellData","text":"Modules = [GridapDistributed]\nPages   = [\"CellData.jl\"]","category":"page"},{"location":"CellData/#GridapDistributed.DistributedCellField","page":"CellData","title":"GridapDistributed.DistributedCellField","text":"\n\n\n\n","category":"type"},{"location":"CellData/#GridapDistributed.DistributedCellPoint","page":"CellData","title":"GridapDistributed.DistributedCellPoint","text":"\n\n\n\n","category":"type"},{"location":"CellData/#GridapDistributed.DistributedDomainContribution","page":"CellData","title":"GridapDistributed.DistributedDomainContribution","text":"\n\n\n\n","category":"type"},{"location":"CellData/#GridapDistributed.DistributedMeasure","page":"CellData","title":"GridapDistributed.DistributedMeasure","text":"\n\n\n\n","category":"type"},{"location":"Visualization/#Visualization","page":"Visualization","title":"Visualization","text":"","category":"section"},{"location":"Visualization/","page":"Visualization","title":"Visualization","text":"Modules = [GridapDistributed]\nPages   = [\"Visualization.jl\"]","category":"page"},{"location":"Visualization/#GridapDistributed.DistributedVisualizationData","page":"Visualization","title":"GridapDistributed.DistributedVisualizationData","text":"\n\n\n\n","category":"type"},{"location":"GridapDistributed/#GridapDistributed","page":"GridapDistributed","title":"GridapDistributed","text":"","category":"section"},{"location":"GridapDistributed/","page":"GridapDistributed","title":"GridapDistributed","text":"Modules = [GridapDistributed]\nPages   = [\"GridapDistributed.jl\"]","category":"page"},{"location":"FESpaces/#FESpaces","page":"FESpaces","title":"FESpaces","text":"","category":"section"},{"location":"FESpaces/","page":"FESpaces","title":"FESpaces","text":"Modules = [GridapDistributed]\nPages   = [\"FESpaces.jl\",\"DivConformingFESpaces.jl\"]","category":"page"},{"location":"FESpaces/#GridapDistributed.DistributedFEFunctionData","page":"FESpaces","title":"GridapDistributed.DistributedFEFunctionData","text":"\n\n\n\n","category":"type"},{"location":"FESpaces/#GridapDistributed.DistributedSingleFieldFEFunction","page":"FESpaces","title":"GridapDistributed.DistributedSingleFieldFEFunction","text":"\n\n\n\n","category":"type"},{"location":"FESpaces/#GridapDistributed.DistributedSingleFieldFESpace","page":"FESpaces","title":"GridapDistributed.DistributedSingleFieldFESpace","text":"\n\n\n\n","category":"type"},{"location":"FESpaces/#GridapDistributed.DistributedSparseMatrixAssembler","page":"FESpaces","title":"GridapDistributed.DistributedSparseMatrixAssembler","text":"\n\n\n\n","category":"type"},{"location":"Algebra/#Algebra","page":"Algebra","title":"Algebra","text":"","category":"section"},{"location":"Algebra/","page":"Algebra","title":"Algebra","text":"Modules = [GridapDistributed]\nPages   = [\"Algebra.jl\"]","category":"page"},{"location":"Algebra/#GridapDistributed.DistributedCounterCOO","page":"Algebra","title":"GridapDistributed.DistributedCounterCOO","text":"\n\n\n\n","category":"type"},{"location":"Algebra/#GridapDistributed.i_am_in-Tuple{MPI.Comm}","page":"Algebra","title":"GridapDistributed.i_am_in","text":"i_am_in(comm::MPIArray)\ni_am_in(comm::DebugArray)\n\nReturns true if the processor is part of the subcommunicator comm.\n\n\n\n\n\n","category":"method"},{"location":"#GridapDistributed.jl","page":"Home","title":"GridapDistributed.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation of the GridapDistributed.jl library.","category":"page"},{"location":"","page":"Home","title":"Home","text":"note: Note\nThese documentation pages are under construction.","category":"page"},{"location":"#Introduction","page":"Home","title":"Introduction","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The ever-increasing demand for resolution and accuracy in mathematical models of physical processes governed by systems of Partial Differential Equations (PDEs)  can only be addressed using fully-parallel advanced numerical discretization methods and scalable solution methods, thus able to exploit the vast amount of computational resources in state-of-the-art supercomputers. To this end, GridapDistributed.jl is a registered software package which provides  fully-parallel distributed memory data structures and associated methods for the Finite Element (FE) numerical solution of PDEs on parallel computers. Thus, it can be run on multi-core CPU desktop computers at small scales, as well as on HPC clusters and supercomputers at medium/large scales. The data structures in GridapDistributed.jl are designed to mirror as far as possible their counterparts in the Gridap.jl Julia software package, while implementing/leveraging most of their abstract interfaces. As a result, sequential Julia scripts written in the high-level Application Programming Interface (API) of Gridap.jl can be used verbatim up to minor adjustments in a parallel distributed memory context using GridapDistributed.jl. This equips end-users with a tool for the development of simulation codes able to solve real-world application problems on massively parallel supercomputers while using a highly expressive, compact syntax, that resembles mathematical notation. This is indeed one of the main advantages of GridapDistributed.jl and a major design goal that we pursue.","category":"page"},{"location":"","page":"Home","title":"Home","text":"In order to scale FE simulations to large core counts, the mesh used to discretize the computational domain on which the PDE is posed must be partitioned (distributed) among the parallel tasks such that each of these only holds a local portion of the global mesh. The same requirement applies to the rest of data structures in the FE simulation pipeline, i.e., FE space, linear system, solvers, data output, etc. The local portion of each task is composed by a set of cells that it owns, i.e., the local cells of the task, and a set of off-processor cells (owned by remote processors) which are in touch with its local cells, i.e., the ghost cells of the task. This overlapped mesh partition is used by GridapDistributed.jl, among others, to exchange data among nearest neighbors, and to glue together global Degrees of Freedom (DoFs) which are sitting on the interface among subdomains. Following this design principle, GridapDistributed.jl provides scalable parallel data structures and associated methods for simple grid handling (in particular, Cartesian-like meshes of arbitrary-dimensional, topologically n-cube domains), FE spaces setup, and distributed linear system assembly. It is in our future plans to provide highly scalable linear and nonlinear solvers tailored for the FE discretization of PDEs (e.g., linear and nonlinear matrix-free geometric multigrid and domain decomposition preconditioners). In the meantime, however, GridapDistributed.jl can be combined with other Julia packages in order to realize the full potential required in real-world applications. These packages and their relation with GridapDistributed.jl are overviewed in the next section. ","category":"page"},{"location":"#Building-blocks-and-composability","page":"Home","title":"Building blocks and composability","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The figure below depicts the relation among GridapDistributed.jl and other packages in the Julia package ecosystem. The interaction of GridapDistributed.jl and its dependencies is mainly designed with separation of concerns in mind towards high composability and modularity. On the one hand, Gridap.jl provides a rich set of abstract types/interfaces suitable for the FE solution of PDEs. It also provides realizations (implementations) of these abstractions tailored to serial/multi-threaded computing environments. GridapDistributed.jl implements these abstractions for parallel distributed-memory computing environments. To this end, GridapDistributed.jl also leverages (uses) the serial realizations in Gridap.jl and associated methods to handle the local portion on each parallel task. (See arrow labels in the figure below.)  On the other hand, GridapDistributed.jl relies on PartitionedArrays.jl in order to handle the parallel execution model (e.g., message-passing via the Message Passing Interface (MPI)), global data distribution layout, and communication among tasks. PartitionedArrays.jl also provides a parallel implementation of partitioned global linear systems (i.e., linear algebra vectors and sparse matrices) as needed in grid-based numerical simulations. While PartitionedArrays.jl is an stand-alone package, segregated from GridapDistributed.jl, it was designed with parallel FE packages such as GridapDistributed.jl in mind. In any case, GridapDistributed.jl is designed so that a different distributed linear algebra library from PartitionedArrays.jl might be used as well, as far as it is able to provide the same functionality. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Image: fig:packages)\nGridapDistributed.jl and its relation to other packages in the Julia package ecosystem. In this diagram, each rectangle represents  a Julia package, while the (directed) arrows represent relations (dependencies) among packages. Both the direction of the arrow and the label attached to the arrows are used to denote the nature of the relation. Thus, e.g., GridapDistributed.jl depends on Gridap.jl and PartitionedArrays.jl , and GridapPETSc depends on Gridap.jl  and PartitionedArrays.jl . Note that, in the diagram, the arrow direction is relevant, e.g., GridapP4est depends on GridapDistributed.jl but not conversely.","category":"page"},{"location":"#How-to-use-this-documentation","page":"Home","title":"How to use this documentation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The first step for new users is to visit the Getting Started page.\nA set of tutorials written as Jupyter notebooks and html pages are available here.\nThe detailed documentation is in the rest of sections of the left hand side menu. (WIP)\nGuidelines for developers of the Gridap project is found in the Gridap wiki page.","category":"page"},{"location":"#Julia-educational-resources","page":"Home","title":"Julia educational resources","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"A basic knowledge of the Julia programming language is needed to use the GridapDistributed package. Here, one can find a list of resources to get started with this programming language.","category":"page"},{"location":"","page":"Home","title":"Home","text":"First steps to learn Julia form the Gridap wiki page.\nOfficial webpage docs.julialang.org\nOfficial list of learning resources julialang.org/learning","category":"page"},{"location":"Adaptivity/#Adaptivity","page":"Adaptivity","title":"Adaptivity","text":"","category":"section"},{"location":"Adaptivity/","page":"Adaptivity","title":"Adaptivity","text":"Modules = [GridapDistributed]\nPages   = [\"Adaptivity.jl\"]","category":"page"},{"location":"Adaptivity/#GridapDistributed.RedistributeGlue","page":"Adaptivity","title":"GridapDistributed.RedistributeGlue","text":"RedistributeGlue\n\nGlue linking two distributions of the same mesh.\n\nnew_parts: Array with the new part IDs (and comms)\nold_parts: Array with the old part IDs (and comms)\nparts_rcv: Array with the part IDs from which each part receives\nparts_snd: Array with the part IDs to which each part sends\nlids_rcv : Local IDs of the entries that are received from each part\nlids_snd : Local IDs of the entries that are sent to each part\nold2new  : Mapping of local IDs from the old to the new mesh\nnew2old  : Mapping of local IDs from the new to the old mesh\n\n\n\n\n\n","category":"type"},{"location":"Adaptivity/#GridapDistributed.redistribute-Tuple{GridapDistributed.DistributedDiscreteModel, Vararg{Any}}","page":"Adaptivity","title":"GridapDistributed.redistribute","text":"Redistributes an DistributedDiscreteModel to optimally    rebalance the loads between the processors.    Returns the rebalanced model and a RedistributeGlue instance. \n\n\n\n\n\n","category":"method"},{"location":"Adaptivity/#GridapDistributed.redistribute_cartesian-Tuple{Union{Nothing, GridapDistributed.GenericDistributedDiscreteModel{Dc, Dp, <:AbstractArray{<:Gridap.Geometry.CartesianDiscreteModel}, B, <:GridapDistributed.DistributedCartesianDescriptor} where {Dc, Dp, B}}, Any, Any}","page":"Adaptivity","title":"GridapDistributed.redistribute_cartesian","text":"redistribute_cartesian(old_model,new_ranks,new_parts)\nredistribute_cartesian(old_model,pdesc::DistributedCartesianDescriptor)\n\nRedistributes a DistributedCartesianDiscreteModel to a new set of ranks and parts.   Only redistributes into a superset of the old_model ranks (i.e. towards more processors).\n\n\n\n\n\n","category":"method"}]
}
