var documenterSearchIndex = {"docs":
[{"location":"Geometry/#Geometry","page":"Geometry","title":"Geometry","text":"","category":"section"},{"location":"Geometry/#MacroDiscreteModels","page":"Geometry","title":"MacroDiscreteModels","text":"These are functionalities to select and globally number the interfaces between processors:","category":"section"},{"location":"Geometry/#GridapDistributed.DistributedDiscreteModel","page":"Geometry","title":"GridapDistributed.DistributedDiscreteModel","text":"\n\n\n\n","category":"type"},{"location":"Geometry/#GridapDistributed.DistributedFaceLabeling","page":"Geometry","title":"GridapDistributed.DistributedFaceLabeling","text":"\n\n\n\n","category":"type"},{"location":"Geometry/#GridapDistributed.DistributedGrid","page":"Geometry","title":"GridapDistributed.DistributedGrid","text":"\n\n\n\n","category":"type"},{"location":"Geometry/#GridapDistributed.DistributedGridTopology","page":"Geometry","title":"GridapDistributed.DistributedGridTopology","text":"\n\n\n\n","category":"type"},{"location":"Geometry/#GridapDistributed.DistributedTriangulation","page":"Geometry","title":"GridapDistributed.DistributedTriangulation","text":"\n\n\n\n","category":"type"},{"location":"Geometry/#GridapDistributed.GenericDistributedDiscreteModel","page":"Geometry","title":"GridapDistributed.GenericDistributedDiscreteModel","text":"\n\n\n\n","category":"type"},{"location":"Geometry/#GridapDistributed.MacroDiscreteModel","page":"Geometry","title":"GridapDistributed.MacroDiscreteModel","text":"struct MacroDiscreteModel{Dc,Dp} <: DistributedDiscreteModel{Dc,Dp}\n\nDistributedModel of the interfaces between the processors. \n\nEach d-interface is given as an agglomeration of d-faces, and a global numbering  is produced for the interfaces. \n\nConstructors:\n\nMacroDiscreteModel(model::DistributedDiscreteModel)\n\nVisualizing the interfaces:\n\nInterfaces have two different numberings. One is local to each processor, and the  other is global and consistent across all processors. To extract and visualize  the interfaces, you can use the following functions:\n\nget_local_face_labeling(macro_model::MacroDiscreteModel{Dc})\nget_global_face_labeling(macro_model::MacroDiscreteModel{Dc})\nwritevtk_local(macro_model::MacroDiscreteModel{Dc},filename::String;vtk_kwargs...)\nwritevtk_global(macro_model::MacroDiscreteModel{Dc},filename::String;vtk_kwargs...)\n\n\n\n\n\n","category":"type"},{"location":"Geometry/#GridapDistributed.classify_interfaces-Union{Tuple{GridapDistributed.DistributedDiscreteModel{Dc}}, Tuple{Dc}} where Dc","page":"Geometry","title":"GridapDistributed.classify_interfaces","text":"Returns a JaggedArray of JaggedArrays such that we have\n\n[interface dimension][interface lid][face dimension] -> interface dfaces lids\n\nI.e we bundle interfaces of the same dimension together (macro-faces, macro-edges, macro-nodes),    and for each interface we return a JaggedArray with the d-faces in the interface.\n\nIf sort_faces is true, the faces in each interface are sorted by gid. If false, the faces   are by default sorted by lid.\n\n\n\n\n\n","category":"method"},{"location":"Geometry/#GridapDistributed.generate_interface_gids-Tuple{Any, Any}","page":"Geometry","title":"GridapDistributed.generate_interface_gids","text":"function generate_interface_gids(nbors,keys) -> gids\n\nGenerates a global numbering for the interfaces, given two input arrays per processor:\n\n- `nbors`: For each interface, the minimum rank of the neighboring processors.\n- `keys`: For each interface, the minimum gid of the faces in the interface.\n\nThen the gids are assigned in the following way:    We iterate over the processors in ascending order. For each processor, we iterate over the local   interfaces. Then: \n\n- If the `nbor` processor of an interface has a higher (or equal) rank than the current processor, it means we\nhaven't assigned a gid to that interface yet. So we assign a new gid to the current interface.\n- If the `nbor` processor of an interface has a lower rank than the current processor, it means we \nalready assigned a gid to that interface (while iterating over `nbor`). So we look for a matching `key`\nin the `keys` array of the `nbor` processor, and assign the same gid to the current interface.\n\n\n\n\n\n","category":"method"},{"location":"Geometry/#GridapDistributed.generate_nbors_and_keys-Union{Tuple{Dc}, Tuple{GridapDistributed.DistributedDiscreteModel{Dc}, Any}} where Dc","page":"Geometry","title":"GridapDistributed.generate_nbors_and_keys","text":"Given a model and a set of local interfaces for each model, returns     - nbors: For each interface, the minimum rank of the neighboring processors.     - keys: For each interface, the minimum gid of the faces in the interface.\n\nOptions: \n\nis_sorted: If true, the keys are assumed to be sorted by gid. This allows some optimization.\ndimensions: List/Set of interface dimensions to be considered. \n\n\n\n\n\n","category":"method"},{"location":"Geometry/#GridapDistributed.get_global_face_labeling-Union{Tuple{GridapDistributed.MacroDiscreteModel{Dc}}, Tuple{Dc}} where Dc","page":"Geometry","title":"GridapDistributed.get_global_face_labeling","text":"Returns a global (consistent) face labeling for the macro model. Requires communication.\n\nThe face labeling contains the following tags:      - Interior_i : Faces which are interior to processor i.     - Interface_j: Faces which belong to interface j (global id).     - Interiors  : Union of all Interior_i tags.     - Interfaces : Union of all Interface_j tags.\n\n\n\n\n\n","category":"method"},{"location":"Geometry/#GridapDistributed.get_local_face_labeling-Union{Tuple{GridapDistributed.MacroDiscreteModel{Dc}}, Tuple{Dc}} where Dc","page":"Geometry","title":"GridapDistributed.get_local_face_labeling","text":"Returns a local face labeling for the macro model. Does not require communication.   WARNING: This is NOT consistent, it is sub-assembled. The same face might have             different labels in different processors.\n\nThe face labeling contains the following tags:      - Interior   : Faces which are interior to the processor.     - Exterior   : Faces which are exterior to the processor.     - Interface_i: Faces which belong to interface i (local id).     - Interfaces : Union of all Interface_i tags.\n\n\n\n\n\n","category":"method"},{"location":"getting-started/#Getting-Started","page":"Getting Started","title":"Getting Started","text":"","category":"section"},{"location":"getting-started/#Installation-requirements","page":"Getting Started","title":"Installation requirements","text":"GridapDistributed is tested on Linux, but it should be also possible to use it on Mac OS and Windows since it is written exclusively in Julia and it only depends on registered Julia packages.","category":"section"},{"location":"getting-started/#Installation","page":"Getting Started","title":"Installation","text":"GridapDistributed is a registered package. Thus, the installation should be straight forward using the Julia's package manager Pkg. To this end, open the Julia REPL (i.e., execute the julia binary), type ] to enter package mode, and install GridapDistributed as follows\n\npkg> add GridapDistributed\n\nYou will also need the PartitionedArrays package. \n\npkg> add PartitionedArrays\n\nIf you want to leverage the satellite packages of GridapDistributed.jl, i.e., GridapPETSc.jl, GridapP4est.jl, and/or GridapGmsh.jl, you have to install them separately as \n\npkg> add GridapPETSc\n\npkg> add GridapGmsh\n\npkg> add GridapP4est\n\nPlease note that these three packages depend on binary builds of the corresponding libraries they wrap (i.e., PETSc, Gmsh, and P4est). By default, they will leverage binary builds available at the Julia package registry. However, one may also use custom installations of these libraries. See the documentation of the corresponding package for more details\n\nFor further information about how to install and manage Julia packages, see the Pkg documentation.","category":"section"},{"location":"getting-started/#Further-steps","page":"Getting Started","title":"Further steps","text":"If you are new to the Gridap ecosystem of packages, we recommend that you first follow the Gridap Tutorials step by step in order to get familiar with the Gridap.jl library. GridapDistributed.jl and Gridap.jl share almost the same high-level API. Therefore, some familiarity with Gridap.jl is highly recommended (if not essential) before starting with GridapDistributed.jl. \n\nIf you are already familiarized with Gridap.jl, we recommend you to start straight away with the following tutorial.","category":"section"},{"location":"MultiField/#MultiField","page":"MultiField","title":"MultiField","text":"","category":"section"},{"location":"MultiField/#GridapDistributed.DistributedMultiFieldFESpace","page":"MultiField","title":"GridapDistributed.DistributedMultiFieldFESpace","text":"\n\n\n\n","category":"type"},{"location":"MultiField/#GridapDistributed.BlockPArray","page":"MultiField","title":"GridapDistributed.BlockPArray","text":"struct BlockPArray{V,T,N,A,B} <: BlockArrays.AbstractBlockArray{T,N}\n\n\n\n\n\n","category":"type"},{"location":"MultiField/#GridapDistributed.BlockPRange","page":"MultiField","title":"GridapDistributed.BlockPRange","text":"struct BlockPRange{A} <: AbstractUnitRange{Int}\n\n\n\n\n\n","category":"type"},{"location":"CellData/#CellData","page":"CellData","title":"CellData","text":"","category":"section"},{"location":"CellData/#GridapDistributed.DistributedCellField","page":"CellData","title":"GridapDistributed.DistributedCellField","text":"\n\n\n\n","category":"type"},{"location":"CellData/#GridapDistributed.DistributedCellPoint","page":"CellData","title":"GridapDistributed.DistributedCellPoint","text":"\n\n\n\n","category":"type"},{"location":"CellData/#GridapDistributed.DistributedDomainContribution","page":"CellData","title":"GridapDistributed.DistributedDomainContribution","text":"\n\n\n\n","category":"type"},{"location":"CellData/#GridapDistributed.DistributedMeasure","page":"CellData","title":"GridapDistributed.DistributedMeasure","text":"\n\n\n\n","category":"type"},{"location":"Visualization/#Visualization","page":"Visualization","title":"Visualization","text":"","category":"section"},{"location":"Visualization/#GridapDistributed.DistributedVisualizationData","page":"Visualization","title":"GridapDistributed.DistributedVisualizationData","text":"\n\n\n\n","category":"type"},{"location":"GridapDistributed/#GridapDistributed","page":"GridapDistributed","title":"GridapDistributed","text":"","category":"section"},{"location":"FESpaces/#FESpaces","page":"FESpaces","title":"FESpaces","text":"","category":"section"},{"location":"FESpaces/#Gridap.FESpaces.ConstantFESpace-Tuple{GridapDistributed.DistributedDiscreteModel}","page":"FESpaces","title":"Gridap.FESpaces.ConstantFESpace","text":"ConstantFESpace(\n  model::DistributedDiscreteModel; \n  constraint_type=:global, \n  kwargs...\n)\n\nDistributed equivalent to ConstantFESpace(model;kwargs...).\n\nWith constraint_type=:global, a single dof is shared by all processors. This creates a global constraint, which is NOT scalable in parallel. Use at your own peril. \n\nWith constraint_type=:local, a single dof is owned by each processor and shared with no one else. This space is locally-constant in each processor, and therefore scalable (but not equivalent to its serial counterpart). \n\n\n\n\n\n","category":"method"},{"location":"FESpaces/#GridapDistributed.DistributedFEFunctionData","page":"FESpaces","title":"GridapDistributed.DistributedFEFunctionData","text":"\n\n\n\n","category":"type"},{"location":"FESpaces/#GridapDistributed.DistributedSingleFieldFEFunction","page":"FESpaces","title":"GridapDistributed.DistributedSingleFieldFEFunction","text":"\n\n\n\n","category":"type"},{"location":"FESpaces/#GridapDistributed.DistributedSingleFieldFESpace","page":"FESpaces","title":"GridapDistributed.DistributedSingleFieldFESpace","text":"\n\n\n\n","category":"type"},{"location":"FESpaces/#GridapDistributed.DistributedSparseMatrixAssembler","page":"FESpaces","title":"GridapDistributed.DistributedSparseMatrixAssembler","text":"\n\n\n\n","category":"type"},{"location":"Algebra/#Algebra","page":"Algebra","title":"Algebra","text":"","category":"section"},{"location":"Algebra/#GridapDistributed.DistributedCounterCOO","page":"Algebra","title":"GridapDistributed.DistributedCounterCOO","text":"\n\n\n\n","category":"type"},{"location":"Algebra/#GridapDistributed.i_am_in-Tuple{MPI.Comm}","page":"Algebra","title":"GridapDistributed.i_am_in","text":"i_am_in(comm::MPIArray)\ni_am_in(comm::DebugArray)\n\nReturns true if the processor is part of the subcommunicator comm.\n\n\n\n\n\n","category":"method"},{"location":"#GridapDistributed.jl","page":"Home","title":"GridapDistributed.jl","text":"Documentation of the GridapDistributed.jl library.\n\nnote: Note\nThese documentation pages are under construction.","category":"section"},{"location":"#Introduction","page":"Home","title":"Introduction","text":"The ever-increasing demand for resolution and accuracy in mathematical models of physical processes governed by systems of Partial Differential Equations (PDEs)  can only be addressed using fully-parallel advanced numerical discretization methods and scalable solution methods, thus able to exploit the vast amount of computational resources in state-of-the-art supercomputers. To this end, GridapDistributed.jl is a registered software package which provides  fully-parallel distributed memory data structures and associated methods for the Finite Element (FE) numerical solution of PDEs on parallel computers. Thus, it can be run on multi-core CPU desktop computers at small scales, as well as on HPC clusters and supercomputers at medium/large scales. The data structures in GridapDistributed.jl are designed to mirror as far as possible their counterparts in the Gridap.jl Julia software package, while implementing/leveraging most of their abstract interfaces. As a result, sequential Julia scripts written in the high-level Application Programming Interface (API) of Gridap.jl can be used verbatim up to minor adjustments in a parallel distributed memory context using GridapDistributed.jl. This equips end-users with a tool for the development of simulation codes able to solve real-world application problems on massively parallel supercomputers while using a highly expressive, compact syntax, that resembles mathematical notation. This is indeed one of the main advantages of GridapDistributed.jl and a major design goal that we pursue.\n\nIn order to scale FE simulations to large core counts, the mesh used to discretize the computational domain on which the PDE is posed must be partitioned (distributed) among the parallel tasks such that each of these only holds a local portion of the global mesh. The same requirement applies to the rest of data structures in the FE simulation pipeline, i.e., FE space, linear system, solvers, data output, etc. The local portion of each task is composed by a set of cells that it owns, i.e., the local cells of the task, and a set of off-processor cells (owned by remote processors) which are in touch with its local cells, i.e., the ghost cells of the task. This overlapped mesh partition is used by GridapDistributed.jl, among others, to exchange data among nearest neighbors, and to glue together global Degrees of Freedom (DoFs) which are sitting on the interface among subdomains. Following this design principle, GridapDistributed.jl provides scalable parallel data structures and associated methods for simple grid handling (in particular, Cartesian-like meshes of arbitrary-dimensional, topologically n-cube domains), FE spaces setup, and distributed linear system assembly. It is in our future plans to provide highly scalable linear and nonlinear solvers tailored for the FE discretization of PDEs (e.g., linear and nonlinear matrix-free geometric multigrid and domain decomposition preconditioners). In the meantime, however, GridapDistributed.jl can be combined with other Julia packages in order to realize the full potential required in real-world applications. These packages and their relation with GridapDistributed.jl are overviewed in the next section. ","category":"section"},{"location":"#Building-blocks-and-composability","page":"Home","title":"Building blocks and composability","text":"The figure below depicts the relation among GridapDistributed.jl and other packages in the Julia package ecosystem. The interaction of GridapDistributed.jl and its dependencies is mainly designed with separation of concerns in mind towards high composability and modularity. On the one hand, Gridap.jl provides a rich set of abstract types/interfaces suitable for the FE solution of PDEs. It also provides realizations (implementations) of these abstractions tailored to serial/multi-threaded computing environments. GridapDistributed.jl implements these abstractions for parallel distributed-memory computing environments. To this end, GridapDistributed.jl also leverages (uses) the serial realizations in Gridap.jl and associated methods to handle the local portion on each parallel task. (See arrow labels in the figure below.)  On the other hand, GridapDistributed.jl relies on PartitionedArrays.jl in order to handle the parallel execution model (e.g., message-passing via the Message Passing Interface (MPI)), global data distribution layout, and communication among tasks. PartitionedArrays.jl also provides a parallel implementation of partitioned global linear systems (i.e., linear algebra vectors and sparse matrices) as needed in grid-based numerical simulations. While PartitionedArrays.jl is an stand-alone package, segregated from GridapDistributed.jl, it was designed with parallel FE packages such as GridapDistributed.jl in mind. In any case, GridapDistributed.jl is designed so that a different distributed linear algebra library from PartitionedArrays.jl might be used as well, as far as it is able to provide the same functionality. \n\n(Image: fig:packages)\nGridapDistributed.jl and its relation to other packages in the Julia package ecosystem. In this diagram, each rectangle represents  a Julia package, while the (directed) arrows represent relations (dependencies) among packages. Both the direction of the arrow and the label attached to the arrows are used to denote the nature of the relation. Thus, e.g., GridapDistributed.jl depends on Gridap.jl and PartitionedArrays.jl , and GridapPETSc depends on Gridap.jl  and PartitionedArrays.jl . Note that, in the diagram, the arrow direction is relevant, e.g., GridapP4est depends on GridapDistributed.jl but not conversely.","category":"section"},{"location":"#How-to-use-this-documentation","page":"Home","title":"How to use this documentation","text":"The first step for new users is to visit the Getting Started page.\nA set of tutorials written as Jupyter notebooks and html pages are available here.\nThe detailed documentation is in the rest of sections of the left hand side menu. (WIP)\nGuidelines for developers of the Gridap project is found in the Gridap wiki page.","category":"section"},{"location":"#Julia-educational-resources","page":"Home","title":"Julia educational resources","text":"A basic knowledge of the Julia programming language is needed to use the GridapDistributed package. Here, one can find a list of resources to get started with this programming language.\n\nFirst steps to learn Julia form the Gridap wiki page.\nOfficial webpage docs.julialang.org\nOfficial list of learning resources julialang.org/learning","category":"section"},{"location":"Adaptivity/#Adaptivity","page":"Adaptivity","title":"Adaptivity","text":"","category":"section"},{"location":"Adaptivity/#GridapDistributed.redistribute-Tuple{GridapDistributed.DistributedDiscreteModel, Vararg{Any}}","page":"Adaptivity","title":"GridapDistributed.redistribute","text":"Redistributes an DistributedDiscreteModel to optimally    rebalance the loads between the processors.    Returns the rebalanced model and a RedistributeGlue instance. \n\n\n\n\n\n","category":"method"},{"location":"Adaptivity/#GridapDistributed.redistribute_cartesian-Tuple{Union{Nothing, GridapDistributed.GenericDistributedDiscreteModel{Dc, Dp, <:AbstractArray{<:Gridap.Geometry.CartesianDiscreteModel}, B, <:GridapDistributed.DistributedCartesianDescriptor} where {Dc, Dp, B}}, Any, Any}","page":"Adaptivity","title":"GridapDistributed.redistribute_cartesian","text":"redistribute_cartesian(old_model,new_ranks,new_parts)\nredistribute_cartesian(old_model,pdesc::DistributedCartesianDescriptor)\n\nRedistributes a DistributedCartesianDiscreteModel to a new set of ranks and parts.   Only redistributes into a superset of the old_model ranks (i.e. towards more processors).\n\n\n\n\n\n","category":"method"},{"location":"Adaptivity/#GridapDistributed.refine_cell_gids-Union{Tuple{Dc}, Tuple{GridapDistributed.DistributedDiscreteModel{Dc}, AbstractArray{<:Gridap.Geometry.DiscreteModel{Dc}}, AbstractArray{<:AbstractArray{Int64}}}} where Dc","page":"Adaptivity","title":"GridapDistributed.refine_cell_gids","text":"refine_cell_gids(\n  cmodel::DistributedDiscreteModel{Dc},\n  fmodels::AbstractArray{<:DiscreteModel{Dc}}\n) where Dc\n\nGiven a coarse model and it's local refined models, returns the gids of the fine model. The gids are computed as follows: \n\nFirst, we create a global numbering for the owned cells by adding an owner-based offset to the local  cell ids (such that cells belonging to the first processor are numbered first). This is  quite standard.\nThe complicated part is making this numeration consistent, i.e communicating gids of the  ghost cells. To do so, each processor selects it's ghost fine cells, and requests their  global ids by sending two keys:\nThe global id of the coarse parent\nThe child id of the fine cell\n\n\n\n\n\n","category":"method"},{"location":"Adaptivity/#GridapDistributed.refine_local_models-Union{Tuple{Dc}, Tuple{GridapDistributed.DistributedDiscreteModel{Dc}, Vararg{Any}}} where Dc","page":"Adaptivity","title":"GridapDistributed.refine_local_models","text":"refine_local_models(cmodel::DistributedDiscreteModel{Dc},args...;kwargs...) where Dc\n\nGiven a coarse model, returns the locally refined models. This is done by \n\nrefining the local models serially\nfiltering out the extra fine layers of ghosts\n\nWe also return the ids of the owned fine cells.\n\nTo find the fine cells we want to keep, we have the following criteria: \n\nGiven a fine cell, it is owned iff  A) It's parent cell is owned\nGiven a fine cell, it is a ghost iff not(A) and  B) It has at least one neighbor with a non-ghost parent\n\nInstead of checking A and B, we do the following: \n\nWe mark fine owned cells by checking A \nIf a cell is owned, we set it's fine neighbors as owned or ghost\n\n\n\n\n\n","category":"method"},{"location":"Adaptivity/#GridapDistributed.RedistributeGlue","page":"Adaptivity","title":"GridapDistributed.RedistributeGlue","text":"RedistributeGlue\n\nGlue linking two distributions of the same mesh.\n\nnew_parts: Array with the new part IDs (and comms)\nold_parts: Array with the old part IDs (and comms)\nparts_rcv: Array with the part IDs from which each part receives\nparts_snd: Array with the part IDs to which each part sends\nlids_rcv : Local IDs of the entries that are received from each part\nlids_snd : Local IDs of the entries that are sent to each part\nold2new  : Mapping of local IDs from the old to the new mesh\nnew2old  : Mapping of local IDs from the new to the old mesh\n\n\n\n\n\n","category":"type"},{"location":"Adaptivity/#GridapDistributed.redistribute-Tuple{PartitionedArrays.PVector, Any}","page":"Adaptivity","title":"GridapDistributed.redistribute","text":"redistribute(v::PVector,new_indices)\nredistribute!(w::PVector,v::PVector,cache)\n\nRedistributes a PVector v to a new partition defined by new_indices.\n\n\n\n\n\n","category":"method"},{"location":"Adaptivity/#GridapDistributed.redistribute_array_by_cells-NTuple{5, Any}","page":"Adaptivity","title":"GridapDistributed.redistribute_array_by_cells","text":"redistribute_array_by_cells(\n  old_lid_to_data, \n  old_cell_to_old_lid,\n  new_cell_to_new_lid,\n  model_new, glue;\n  T = Int32, reverse=false,\n)\n\nRedistributes an array using the old machinery. This is used to create the new index partitions that the new machinery requires.\n\nTakes in: \n\nold_lid_to_data: The data to redistribute, indexed by local IDs in the old communicator.\nold_cell_to_old_lid: The mapping from cells to local data IDs in the old communicator.\nnew_cell_to_new_lid: The mapping from cells to local data IDs in the new communicator.\nmodel_new: The model in the new communicator.\nglue: The RedistributeGlue glue.\n\nReturns: \n\nnew_lid_to_old_data: The redistributed data, indexed by local IDs in the new communicator.\n\n\n\n\n\n","category":"method"},{"location":"Adaptivity/#GridapDistributed.redistribute_indices-NTuple{5, Any}","page":"Adaptivity","title":"GridapDistributed.redistribute_indices","text":"redistribute_indices(\n  old_ids,\n  old_cell_to_old_lid,\n  new_cell_to_new_lid,\n  model_new, glue;\n  reverse=false,\n)\n\nRedistributes an index partition from an old communicator to a new one, ensuring that  the global ids coincide in both partitions.\n\nTakes in:\n\nold_ids: The old index partition, providing the local-to-global ID map in the old communicator.\nold_cell_to_old_lid: The mapping from cells to local IDs in the old communicator.\nnew_cell_to_new_lid: The mapping from cells to local IDs in the new communicator.\nmodel_new: The model in the new communicator.\nglue: The RedistributeGlue glue.\n\nReturns:\n\nold_ids: The old index partition, but defined in the new communicator. It has empty             entries for the parts that are not in the old communicator.\nred_old_ids: The redistributed index partition, defined in the new communicator.\n\n\n\n\n\n","category":"method"},{"location":"Adaptivity/#GridapDistributed.redistribution_local_indices-Tuple{Any, Any}","page":"Adaptivity","title":"GridapDistributed.redistribution_local_indices","text":"redistribution_local_indices(indices, indices_red) -> (lids_snd, lids_rcv)\n\nReturns the local indices to be communicated when redistributing from indices to indices_red.\n\nCAREFUL: Unlike for the assembly operation, these snd/rcv indices are NOT symmetric.\n\n\n\n\n\n","category":"method"}]
}
